{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9dd2e66-e8e0-447e-801a-c044d9098fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私\tワタクシ\tワタクシ\t私-代名詞\t代名詞\t\t\t0\n",
      "は\tワ\tハ\tは\t助詞-係助詞\t\t\t\n",
      "日本\tニッポン\tニッポン\t日本\t名詞-固有名詞-地名-国\t\t\t3\n",
      "語\tゴ\tゴ\t語\t名詞-普通名詞-一般\t\t\t1\n",
      "を\tオ\tヲ\tを\t助詞-格助詞\t\t\t\n",
      "勉強\tベンキョー\tベンキョウ\t勉強\t名詞-普通名詞-サ変可能\t\t\t0\n",
      "し\tシ\tスル\t為る\t動詞-非自立可能\tサ行変格\t連用形-一般\t0\n",
      "て\tテ\tテ\tて\t助詞-接続助詞\t\t\t\n",
      "い\tイ\tイル\t居る\t動詞-非自立可能\t上一段-ア行\t連用形-一般\t0\n",
      "ます\tマス\tマス\tます\t助動詞\t助動詞-マス\t終止形-一般\t\n",
      "。\t\t\t。\t補助記号-句点\t\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MeCabとPythonを使用した日本語テキスト分析入門\n",
    "事前準備（必要なパッケージをインストール）\n",
    "\n",
    "pip install mecab-python3\n",
    "pip install unidic-lite\n",
    "\"\"\"\n",
    "import MeCab\n",
    "\n",
    "# MeCab Taggerオブジェクトを作成\n",
    "mecab = MeCab.Tagger()\n",
    "\n",
    "# 簡単な日本語の文を分析\n",
    "text = \"私は日本語を勉強しています。\"\n",
    "result = mecab.parse(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040b04fa-ed27-4c34-8bdf-a7474daa5529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トークン, 解析結果\n",
      "====== , ==================================================\n",
      "私, 代名詞,*,*,*,*,*,ワタクシ,私-代名詞,私,ワタクシ,私,ワタクシ,和,*,*,*,*,ワタクシ,ワタクシ,ワタクシ,ワタクシ,*,*,0,*,*\n",
      "は, 助詞,係助詞,*,*,*,*,ハ,は,は,ワ,は,ワ,和,*,*,*,*,ハ,ハ,ハ,ハ,*,*,*,\"動詞%F2@0,名詞%F1,形容詞%F2@-1\",*\n",
      "日本, 名詞,固有名詞,地名,国,*,*,ニッポン,日本,日本,ニッポン,日本,ニッポン,固,*,*,*,*,ニッポン,ニッポン,ニッポン,ニッポン,*,*,3,*,*\n",
      "語, 名詞,普通名詞,一般,*,*,*,ゴ,語,語,ゴ,語,ゴ,漢,*,*,*,*,ゴ,ゴ,ゴ,ゴ,*,*,1,C3,*\n",
      "を, 助詞,格助詞,*,*,*,*,ヲ,を,を,オ,を,オ,和,*,*,*,*,ヲ,ヲ,ヲ,ヲ,*,*,*,\"動詞%F2@0,名詞%F1,形容詞%F2@-1\",*\n",
      "勉強, 名詞,普通名詞,サ変可能,*,*,*,ベンキョウ,勉強,勉強,ベンキョー,勉強,ベンキョー,漢,*,*,*,*,ベンキョウ,ベンキョウ,ベンキョウ,ベンキョウ,*,*,0,C2,*\n",
      "し, 動詞,非自立可能,*,*,サ行変格,連用形-一般,スル,為る,し,シ,する,スル,和,*,*,*,*,シ,スル,シ,スル,*,*,0,C5,*\n",
      "て, 助詞,接続助詞,*,*,*,*,テ,て,て,テ,て,テ,和,*,*,*,*,テ,テ,テ,テ,*,*,*,\"動詞%F1,形容詞%F2@-1\",*\n",
      "い, 動詞,非自立可能,*,*,上一段-ア行,連用形-一般,イル,居る,い,イ,いる,イル,和,*,*,*,*,イ,イル,イ,イル,*,*,0,C4,*\n",
      "ます, 助動詞,*,*,*,助動詞-マス,終止形-一般,マス,ます,ます,マス,ます,マス,和,*,*,*,*,マス,マス,マス,マス,*,*,*,動詞%F4@1,*\n",
      "。, 補助記号,句点,*,*,*,*,,。,。,,。,,記号,*,*,*,*,,,,,*,*,*,*,*\n"
     ]
    }
   ],
   "source": [
    "nodes = mecab.parseToNode(text)\n",
    "print(\"トークン, 解析結果\")\n",
    "print(\"=\" * 6, \",\" , \"=\" * 50)\n",
    "\n",
    "while nodes:\n",
    "    if nodes.surface != \"\":\n",
    "        print(f\"{nodes.surface}, {nodes.feature}\")\n",
    "    nodes = nodes.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3f95d3-2e53-4bf8-bbec-7dad6c9e3982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私', 'は', '日本', '語', 'を', '勉強', 'し', 'て', 'い', 'ます', '。']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "テキストのトークン化\n",
    "\"\"\"\n",
    "def tokenize(text):\n",
    "    mecab = MeCab.Tagger()\n",
    "    nodes = mecab.parseToNode(text)\n",
    "    tokens = []\n",
    "    while nodes:\n",
    "        if nodes.surface != \"\":\n",
    "            tokens.append(nodes.surface)\n",
    "        nodes = nodes.next\n",
    "    return tokens\n",
    "\n",
    "# 使用例\n",
    "text = \"私は日本語を勉強しています。\"\n",
    "tokens = tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f1b535-6c37-4662-8b2f-45bd9f423d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('私', '代名詞'), ('は', '助詞'), ('日本', '名詞'), ('語', '名詞'), ('を', '助詞'), ('勉強', '名詞'), ('し', '動詞'), ('て', '助詞'), ('い', '動詞'), ('ます', '助動詞'), ('。', '補助記号')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "品詞情報の抽出\n",
    "\"\"\"\n",
    "def tokenize_with_pos(text):\n",
    "    mecab = MeCab.Tagger()\n",
    "    nodes = mecab.parseToNode(text)\n",
    "    tokens = []\n",
    "    while nodes:\n",
    "        if nodes.surface != \"\":\n",
    "            tokens.append((nodes.surface, nodes.feature.split(',')[0]))\n",
    "        nodes = nodes.next\n",
    "    return tokens\n",
    "\n",
    "# 使用例\n",
    "text = \"私は日本語を勉強しています。\"\n",
    "tokens_with_pos = tokenize_with_pos(text)\n",
    "print(tokens_with_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c20e5522-4214-4e9d-a4bc-233cee4b9b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['日本', '語', '勉強']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "特定の品詞のフィルタリング\n",
    "\"\"\"\n",
    "def filter_pos(tokens, pos_list):\n",
    "    return [token for token, pos in tokens if pos in pos_list]\n",
    "\n",
    "# 使用例\n",
    "text = \"私は日本語を勉強しています。\"\n",
    "tokens_with_pos = tokenize_with_pos(text)\n",
    "nouns = filter_pos(tokens_with_pos, ['名詞'])\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bc96ce6-9bec-46fe-ae9e-e5adc74a927c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私-代名詞', 'は', '日本', '語', 'を', '勉強', '為る', 'て', '居る', 'ます', '。']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "基本形の抽出\n",
    "\"\"\"\n",
    "def get_base_forms(text):\n",
    "    mecab = MeCab.Tagger()\n",
    "    nodes = mecab.parseToNode(text)\n",
    "    base_forms = []\n",
    "    while nodes:\n",
    "        if nodes.surface != \"\":\n",
    "            features = nodes.feature.split(',')\n",
    "            base_forms.append(features[7] if len(features) > 8 else nodes.surface)\n",
    "        nodes = nodes.next\n",
    "    return base_forms\n",
    "\n",
    "# 使用例\n",
    "text = \"私は日本語を勉強しています。\"\n",
    "base_forms = get_base_forms(text)\n",
    "print(base_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d5317ae-3703-4aa0-a98c-17ecb48895ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'私': 2, 'は': 2, '日本': 2, '語': 2, 'を': 2, 'ます': 2, '。': 2, '勉強': 1, 'し': 1, 'て': 1, 'い': 1, '毎日': 1, '話し': 1})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "単語の頻度カウント\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "\n",
    "def count_word_frequencies(text):\n",
    "    tokens = tokenize(text)\n",
    "    return Counter(tokens)\n",
    "\n",
    "# 使用例\n",
    "text = \"私は日本語を勉強しています。私は毎日日本語を話します。\"\n",
    "word_frequencies = count_word_frequencies(text)\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bd1b179-c285-4bc9-9102-b36083e4d986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['東京', '山田']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "固有表現の抽出\n",
    "\"\"\"\n",
    "def extract_named_entities(text):\n",
    "    mecab = MeCab.Tagger()\n",
    "    nodes = mecab.parseToNode(text)\n",
    "    named_entities = []\n",
    "    current_ne = []\n",
    "    while nodes:\n",
    "        if nodes.surface != \"\":\n",
    "            features = nodes.feature.split(',')\n",
    "            if features[1] == '固有名詞':\n",
    "                current_ne.append(nodes.surface)\n",
    "            else:\n",
    "                if current_ne:\n",
    "                    named_entities.append(''.join(current_ne))\n",
    "                    current_ne = []\n",
    "        nodes = nodes.next\n",
    "    if current_ne:\n",
    "        named_entities.append(''.join(current_ne))\n",
    "    return named_entities\n",
    "\n",
    "# 使用例\n",
    "text = \"私は東京で山田さんと会いました。\"\n",
    "named_entities = extract_named_entities(text)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d193e0-bf0b-4223-86cb-a52cdcb2682b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "また、敬語システムも複雑で、状況に応じて適切な言葉遣いを選ぶ必要があります。\n",
      "しかし、日本語を学ぶことで、日本の豊かな文化や歴史に触れることができます。\n",
      "多くの外国人が日本語に魅力を感じ、学習に取り組んでいます。\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "テキスト要約\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "\n",
    "def simple_summarize(text, num_sentences=3):\n",
    "    sentences = text.split('。')\n",
    "    word_frequencies = defaultdict(int)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            word_frequencies[token] += 1\n",
    "    \n",
    "    sentence_scores = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        score = sum(word_frequencies[token] for token in tokenize(sentence))\n",
    "        sentence_scores.append((i, score))\n",
    "    \n",
    "    top_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)[:num_sentences]\n",
    "    top_sentences = sorted(top_sentences, key=lambda x: x[0])\n",
    "    \n",
    "    summary = '。'.join(sentences[i] for i, _ in top_sentences) + '。'\n",
    "    return summary\n",
    "\n",
    "# 使用例\n",
    "text = \"\"\"\n",
    "日本語は世界で最も難しい言語の一つだと言われています。\n",
    "漢字、ひらがな、カタカナという3つの文字体系があります。\n",
    "また、敬語システムも複雑で、状況に応じて適切な言葉遣いを選ぶ必要があります。\n",
    "しかし、日本語を学ぶことで、日本の豊かな文化や歴史に触れることができます。\n",
    "多くの外国人が日本語に魅力を感じ、学習に取り組んでいます。\n",
    "日本語の習得は確かに難しいですが、努力次第で必ず上達します。\n",
    "\"\"\"\n",
    "\n",
    "summary = simple_summarize(text, num_sentences=3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139807f-638e-4074-8389-2ef5afae05e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
